{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown==4.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 1p-rDwR--uHZb-_YKvynDKGBcKQJdYY-d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip lol_dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "patch_size = 128\n",
    "base_feature_maps = 32\n",
    "n_epochs = 100\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path  = os.getcwd()\n",
    "root_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename(os.path.join(root_path,'lol_dataset/our485'),os.path.join(root_path,'lol_dataset/train'))\n",
    "os.rename(os.path.join(root_path,'lol_dataset/eval15'),os.path.join(root_path,'lol_dataset/val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_dir = os.path.join(root_path,'lol_dataset/train')\n",
    "high_files = os.listdir(os.path.join(rgb_dir, 'high'))\n",
    "low_files = os.listdir(os.path.join(rgb_dir, 'low'))\n",
    "high_files = [os.path.join(rgb_dir, 'high', file_name)for file_name in high_files if file_name.endswith('.png')]\n",
    "low_files = [os.path.join(rgb_dir, 'low', file_name)for file_name in low_files if file_name.endswith('.png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "columns = 4\n",
    "rows = 4\n",
    "for i in range(1, columns*rows +1):\n",
    "    img = cv2.cvtColor(cv2.imread(high_files[i]),cv2.COLOR_BGR2RGB)\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "columns = 4\n",
    "rows = 4\n",
    "for i in range(1, columns*rows +1):\n",
    "    img = cv2.cvtColor(cv2.imread(low_files[i]),cv2.COLOR_BGR2RGB)\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LolDataLoader(Dataset):\n",
    "    def __init__(self, rgb_dir, transform=None):\n",
    "        super(LolDataLoader, self).__init__()\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        high_files = sorted(os.listdir(os.path.join(rgb_dir, 'high')))\n",
    "        low_files = sorted(os.listdir(os.path.join(rgb_dir, 'low')))\n",
    "        \n",
    "        self.high_filenames = [os.path.join(rgb_dir, 'high', x) for x in high_files if self.is_png_file(x)]\n",
    "        self.low_filenames = [os.path.join(rgb_dir, 'low', x)  for x in low_files if self.is_png_file(x)]\n",
    "\n",
    "        self.tar_size = len(self.high_filenames)  # get the size of target\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tar_size\n",
    "    def is_png_file(self,filename):\n",
    "        return any(filename.endswith(extension) for extension in [\".png\"])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        high = Image.open(self.high_filenames[index])\n",
    "        low = Image.open(self.low_filenames[index])\n",
    "\n",
    "        high = np.asarray(high)\n",
    "        low = np.asarray(low)\n",
    "\n",
    "\n",
    "        high_filename = os.path.split(self.high_filenames[index])[-1]\n",
    "        low_filename = os.path.split(self.low_filenames[index])[-1]\n",
    "        comb_im = np.concatenate([high,low],axis=-1)\n",
    "        if self.transform is not None:\n",
    "            comb_im = self.transform(comb_im)\n",
    "        comb_im = torch.split(comb_im,split_size_or_sections = 3,dim = 0)\n",
    "        high, low = comb_im[0],comb_im[1]\n",
    "        return high, low, high_filename, low_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation/Augmentations to be applied on Trainig and Validation Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = T.Compose([T.RandomCrop((patch_size,patch_size)),T.ToTensor()])\n",
    "val_transforms = T.Compose([T.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loader for Trainig and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = T.Compose([T.ToTensor(),T.RandomCrop((128,128))])\n",
    "val_transforms = T.Compose([T.ToTensor()])\n",
    "train_dataset = LolDataLoader(rgb_dir=os.path.join(root_path,'lol_dataset/train'),transform=train_transforms)\n",
    "val_dataset = LolDataLoader(rgb_dir=os.path.join(root_path,'lol_dataset/val'),transform=val_transforms)\n",
    "train_data_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for count,(inp,gt,i_name,g_name) in enumerate(val_data_loader):\n",
    "#     inp_images = [(inp[i,...].permute(1,2,0)*255).cpu().numpy() for i in range(inp.shape[0])]\n",
    "#     gt_images = [(gt[i,...].permute(1,2,0)*255).cpu().numpy() for i in range(gt.shape[0])]\n",
    "#     inps = np.concatenate(inp_images,axis=1)\n",
    "#     gts = np.concatenate(gt_images,axis=1)\n",
    "#     final = np.concatenate([inps,gts],axis=0)\n",
    "#     cv2.imwrite(f'{count}.jpg',cv2.cvtColor(final,cv2.COLOR_RGB2BGR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Development using Sequential Module of Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Relight = UNet(n_channels=3)\n",
    "Relight = Relight.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = nn.Sequential(nn.Conv2d(in_channels=3,out_channels=base_feature_maps,kernel_size = 3,padding='same'),nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=base_feature_maps,out_channels=base_feature_maps,kernel_size = 3,padding='same'),nn.ReLU(),nn.MaxPool2d(kernel_size=2),\n",
    "        nn.Conv2d(in_channels=base_feature_maps,out_channels=base_feature_maps*2,kernel_size = 3,padding='same'),nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=base_feature_maps*2,out_channels=base_feature_maps*2,kernel_size = 3,padding='same'),nn.ReLU(),nn.MaxPool2d(kernel_size=2),\n",
    "        nn.Conv2d(in_channels=base_feature_maps*2,out_channels=base_feature_maps*4,kernel_size = 3,padding='same'),nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=base_feature_maps*4,out_channels=base_feature_maps*4,kernel_size = 3,padding='same'),nn.ReLU(),nn.MaxPool2d(kernel_size=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Decoder = nn.Sequential(nn.Conv2d(in_channels=base_feature_maps*4,out_channels=base_feature_maps*4,kernel_size = 3,padding='same'),nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=base_feature_maps*4,out_channels=base_feature_maps*4,kernel_size = 3,padding='same'),nn.ReLU(),\n",
    "        nn.ConvTranspose2d(in_channels=base_feature_maps*4,out_channels=base_feature_maps*2,kernel_size=2,stride=2),\n",
    "        nn.Conv2d(in_channels=base_feature_maps*2,out_channels=base_feature_maps*2,kernel_size = 3,padding='same'),nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=base_feature_maps*2,out_channels=base_feature_maps*2,kernel_size = 3,padding='same'),nn.ReLU(),\n",
    "        nn.ConvTranspose2d(in_channels=base_feature_maps*2,out_channels=base_feature_maps,kernel_size=2,stride=2),\n",
    "        nn.Conv2d(in_channels=base_feature_maps,out_channels=base_feature_maps,kernel_size = 3,padding='same'),nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=base_feature_maps,out_channels=base_feature_maps,kernel_size = 3,padding='same'),nn.ReLU(),\n",
    "        nn.ConvTranspose2d(in_channels=base_feature_maps,out_channels=base_feature_maps,kernel_size=2,stride=2),\n",
    "        nn.Conv2d(in_channels=base_feature_maps,out_channels=3,kernel_size = 3,padding='same'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Relight Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Relight = nn.Sequential(Encoder,Decoder)\n",
    "Relight = Relight.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Optimizer Intialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion  = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(Relight.parameters(),lr =1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/PSNR.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSNR(tar_img, prd_img):\n",
    "    mse = torch.mean((torch.clamp(tar_img,0,1) - torch.clamp(prd_img,0,1)) ** 2)\n",
    "    ps = 20 * torch.log10(1.0 / torch.sqrt(mse))\n",
    "    return ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    running_train_loss = 0.0\n",
    "    running_psnr  = 0.0\n",
    "    Relight.train()\n",
    "    for high,low,_,_ in tqdm(train_data_loader):\n",
    "        high = high.to(device)\n",
    "        low = low.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = Relight(low)\n",
    "            loss = criterion(outputs,high)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        running_psnr += PSNR(high,outputs)\n",
    "        running_train_loss += loss.item()\n",
    "    epoch_loss = running_train_loss / (len(train_data_loader))\n",
    "    epoch_psnr = running_psnr / (len(train_data_loader))\n",
    "    return epoch_loss,epoch_psnr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation():\n",
    "    running_val_loss = 0.0\n",
    "    running_psnr  = 0.0\n",
    "    Relight.eval()\n",
    "    for high,low,_,_ in tqdm(val_data_loader):\n",
    "        high = high.to(device)\n",
    "        low = low.to(device)\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = Relight(low)\n",
    "            loss = criterion(outputs,high)\n",
    "        running_val_loss += loss.item()\n",
    "        running_psnr += PSNR(high,outputs)\n",
    "    epoch_loss = running_val_loss / (len(val_data_loader))\n",
    "    epoch_psnr = running_psnr / (len(val_data_loader))\n",
    "    return epoch_loss,epoch_psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    train_loss,train_psnr = train()\n",
    "    val_loss,val_psnr = validation()\n",
    "    print(f'Epoch number = {epoch+1} l1 train loss = {train_loss},train psnr = {train_psnr}')\n",
    "    print(f'Epoch number = {epoch+1} l1 val loss = {val_loss}, val psnr = {val_psnr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Relight.state_dict(),'relight.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Relight.load_state_dict(torch.load('relight.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Relight.eval()\n",
    "high,low,_,_ = iter(val_data_loader)\n",
    "high = high[0].to(device)\n",
    "low = low[0].to(device)\n",
    "with torch.set_grad_enabled(False):\n",
    "    outputs = Relight(low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Relight.eval()\n",
    "for count,(low,high,_,_) in enumerate(val_data_loader):\n",
    "    high = high.to(device)\n",
    "    low = low.to(device)\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = Relight(low)\n",
    "    inp_images = [(low[i,...].permute(1,2,0)).cpu().numpy() for i in range(low.shape[0])]\n",
    "    gt_images = [(high[i,...].permute(1,2,0)).cpu().numpy() for i in range(high.shape[0])]\n",
    "    pred_images = [(outputs[i,...].permute(1,2,0)).cpu().numpy() for i in range(outputs.shape[0])]\n",
    "    inps = np.concatenate(inp_images,axis=1)\n",
    "    gts = np.concatenate(gt_images,axis=1)\n",
    "    preds = np.concatenate(pred_images,axis=1)\n",
    "    final = np.concatenate([inps,preds,gts],axis=0)\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    plt.imshow(final)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('torchenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13 (default, Mar 29 2022, 02:18:16) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0c9fb112d4624b5e4fa7b873160be5d487c9f44e6948be985e0928a4033755e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
